{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K4cp3rski/ML_FUW/blob/master/Cybi%C5%84ski_02_zadanie_domowe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYtv4ZSbTx5X"
      },
      "source": [
        "# Praca domowa I, zadanie II\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaX60EphVC74"
      },
      "source": [
        "## Tre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4_wGX3_coFa"
      },
      "source": [
        "### Wstp fabularny"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJlu3kmQchcE"
      },
      "source": [
        "Wyobra藕 sobie, 偶e jeste pracownikiem w firmie sprzedajcej kompleksow usug tworzenia wizerunk贸w medialnych. Oddzia, w kt贸rym pracujesz obsuguje wa偶nego klienta dziaajcego w bran偶y gier i usug cyfrowych.\n",
        "\n",
        "Twoim zadaniem jest przygotowa model uczenia maszynowego, kt贸ry okrela bdzie nastawienie emocjonalne post贸w z Twittera. Zesp贸 odpowiadajcy za zbieranie danych wanie dostarczy zestaw danych dla Ciebie.\n",
        "\n",
        "Do tej pory klasyfikowaniem nastroj贸w z twitt贸w zajmowa si zesp贸 ekspert贸w. Rozwizanie takie jest bardzo wolne i drogie, a dokadno ekspert贸w wynosi tylko 95%. Dlatego zarzd firmy zleci wdro偶enie modelu uczenia maszynowego.\n",
        "\n",
        "Tw贸j model stanowi bdzie jedynie cz wikszego produktu oferowanego przez Twoj firm. Wyniki Twojego modelu bd bezporednio wykorzystywane przez nastpny zesp贸, kt贸rego zadaniem jest przygotowa kolejny model uczenia maszynowego przewidujcy reakcje opinii publicznej na posty klienta.\n",
        "\n",
        "Prace zespou, kt贸ry korzysta bdzie z Twojego modelu s ju偶 bardzo zaawansowane, dlatego nie mo偶e on pozwoli sobie na 偶adne dodatkowe zmiany w swoim projekcie. Absolutnie konieczne jest, aby Tw贸j model przyporzdkowywa posty do jednej z trzech klas 'Positive', 'Negative', 'Neutral' lub analogicznych. Posty nie na temat powinny by klasyfikowane jako 'Neutral'.\n",
        "\n",
        "Notebook z Twoim projektem bdzie oglda Tw贸j szef, wic koniecznie zadbaj, 偶eby znalazy si w nim najwa偶niejsze przemylenia, a rysunki byy adne.\n",
        "\n",
        "Powodzenia "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRk5odT-VpHl"
      },
      "source": [
        "### Polecenia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqep_SopVL6B"
      },
      "source": [
        "1. Wstpna obr贸bka danych:\n",
        "\n",
        " - zaaduj zbi贸r treningowy i testowy,\n",
        " - usu wiersze o brakujcych elementach,\n",
        " - w kolumnie `sentiment` zamie wartoci `'Irrelevant'` na `'Neutral'`.\n",
        "\n",
        "1. Wykonaj wizualizacje danych:\n",
        "\n",
        " - histogram temat贸w twitt贸w (`entity`),\n",
        " - histogram nastawie (`sentiment`),\n",
        " - najczciej padajcych s贸w w treci twitt贸w (`content`).\n",
        "\n",
        "1. Przygotuj dane:\n",
        "\n",
        " - przygotuj zbi贸r cech poprzez wektoryzacje kolumny `content`, \n",
        " - przygotuj etykiety poprzez zakodowanie tekstowych wartoci w kolumnie `sentiment` do postaci liczbowej.\n",
        "\n",
        "  Nastpnie wytrenuj naiwny model bayesowski. Sprawd藕 dziaanie modelu na kilku wasnorcznie napisanych wiadomociach. \n",
        "\n",
        "1. Wytrenuj modele:\n",
        " - naiwny bayesowski,\n",
        " - liniowy SVM,\n",
        " - regresji logistycznej,\n",
        " - drzewo decyzyjne.\n",
        "\n",
        "  Sprawd藕 model na danych treningowych (walidacja krzy偶owa) i testowych, nastpnie wybierz najlepszy model. Uzasadnij sw贸j wyb贸r.\n",
        "  \n",
        "1. Zesp贸 ekspert贸w rcznie klasyfikuje dane z dokadnoci 95%. Por贸wnaj z nimi sw贸j model i napisz jakie s przewagi Twojego modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZpaDghkU_B7"
      },
      "source": [
        "### Zbi贸r danych\n",
        "\n",
        "Zbi贸r danych zosta przygotowany na podstawie zbioru [Twitter Sentiment Analysis](https://www.kaggle.com/jp797498e/twitter-entity-sentiment-analysis) i skada si z dw贸ch plik贸w:\n",
        "-  `twitter_training.csv` - zbi贸r treningowy,\n",
        "- `twitter_validation.csv` - zbi贸r testowy.\n",
        "\n",
        "Archiwum z plikami mo偶na pobra z [dysku google](https://drive.google.com/file/d/1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB/view?usp=sharing) lub odkomentowujc poni偶sze linie:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "lrlti4Zk4Lhf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VlvbsKdVU0hz"
      },
      "outputs": [],
      "source": [
        "# ! pip install gdown\n",
        "# ! gdown https://drive.google.com/uc?id=1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB\n",
        "# ! unzip twitter.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "95mTOShAjGji",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d79a6541-2b6a-44fb-f91a-8c13e2d78591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (366 kB)\n",
            "\u001b[K     || 366 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from wordcloud) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\n",
            "Installing collected packages: wordcloud\n",
            "  Attempting uninstall: wordcloud\n",
            "    Found existing installation: wordcloud 1.5.0\n",
            "    Uninstalling wordcloud-1.5.0:\n",
            "      Successfully uninstalled wordcloud-1.5.0\n",
            "Successfully installed wordcloud-1.8.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     || 1.5 MB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     || 749 kB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Installing collected packages: regex, nltk\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.7 regex-2021.11.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "regex"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
            "\u001b[K     || 24.8 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "Successfully installed scikit-learn-1.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     || 11.3 MB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     || 15.7 MB 15.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U wordcloud\n",
        "!pip install -U nltk\n",
        "!pip install -U scikit-learn\n",
        "!pip install -U pandas\n",
        "!pip install -U seaborn\n",
        "!pip install -U numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGrsOK5pT1Xw"
      },
      "source": [
        "# Rozwizanie"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impoty \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import cross_validate\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from scipy.sparse import hstack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TERrt4LEZdp",
        "outputId": "612c0dce-0de9-4aac-a290-0539d70cd7fa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adowanie danych"
      ],
      "metadata": {
        "id": "de84o_Yt-3LL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lQ0m63_gU41z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1356d2-f2d2-4ee7-b694-ffbeea4719a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zainstalowana wersja scikit-learn: 1.0.1.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "print('Zainstalowana wersja scikit-learn: {}.'.format(sklearn.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10) # aby wykresy w Colabie byy wiksze\n",
        "\n",
        "import numpy as np\n",
        "from scipy import diag, interp\n",
        "from itertools import cycle\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h6Ykp7XDRlF5"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('twitter_training.csv')\n",
        "test_data = pd.read_csv('twitter_validation.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7rWWbBA9wTr"
      },
      "source": [
        "Jak widzimy globalnie (Obrazek 1) klasy s w miare zbalansowane, liczebno nie r贸偶ni siwicej jak 2-krotnie. W zwizku z tym wydaje si, 偶e nie ma potrzeby sztucznego wyr贸wnywania ich liczebnoci i mo偶na przej do dalszego etapu preprocessingu danych"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing danych"
      ],
      "metadata": {
        "id": "uNbac6Dd_Do3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zmieniamy labelki na integerowe"
      ],
      "metadata": {
        "id": "FYYpHtacAvIs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aMIHVxWiR4s8"
      },
      "outputs": [],
      "source": [
        "# Przydzielamy klasom integerowe labelki\n",
        "\n",
        "# Neutral = Class 0\n",
        "train_data.loc[train_data['sentiment'] == 'Positive', 'sentiment'] = 0\n",
        "# Positive = Class 1\n",
        "train_data.loc[train_data['sentiment'] == 'Neutral', 'sentiment'] = 1\n",
        "# Negative = Class 2\n",
        "train_data.loc[train_data['sentiment'] == 'Negative', 'sentiment'] = 2\n",
        "# Irrelevant = Class 1\n",
        "train_data.loc[train_data['sentiment'] == 'Irrelevant', 'sentiment'] = 1\n",
        "\n",
        "# A teraz to samo dla zbiotu testowego\n",
        "\n",
        "# Neutral = Class 0\n",
        "test_data.loc[test_data['sentiment'] == 'Positive', 'sentiment'] = 0\n",
        "# Positive = Class 1\n",
        "test_data.loc[test_data['sentiment'] == 'Neutral', 'sentiment'] = 1\n",
        "# Negative = Class 2\n",
        "test_data.loc[test_data['sentiment'] == 'Negative', 'sentiment'] = 2\n",
        "# Irrelevant = Class 1\n",
        "test_data.loc[test_data['sentiment'] == 'Irrelevant', 'sentiment'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vYH-7SsoXZy1"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.drop(columns='id')\n",
        "test_data = test_data.drop(columns='id')\n",
        "\n",
        "train_data = train_data.rename(columns={'sentiment':'Class'})\n",
        "test_data = test_data.rename(columns={'sentiment':'Class'})\n",
        "\n",
        "# Pozbywamy si wierszy z niepenymi informacjani (NaN)\n",
        "train_data = train_data.dropna(axis='rows', how='all', thresh=int(train_data.shape[1]))\n",
        "test_data = test_data.dropna(axis='rows', how='all', thresh=int(test_data.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wizualizacja danych, oraz najczciej wystpujcych s贸w w tweetach danych kategorii"
      ],
      "metadata": {
        "id": "dkfna3Sm_SYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Wizualizacja danych\n",
        "\n",
        "# f = sns.countplot(x='sentiment', data=train_data)\n",
        "# h = sns.catplot(y='entity', hue='sentiment', data=train_data, kind='count', height=20, aspect=1)\n",
        "# g = sns.catplot(x='sentiment', col='entity' , col_wrap=4, kind='count', data=train_data, height=4.5, aspect=1.2)\n",
        "# i = sns.catplot(y='entity', data=train_data, height=20, kind='count')\n",
        "\n",
        "# (g.set_axis_labels(\"\", \"Tweet count\")\n",
        "# .set_xticklabels([\"Positive\", \"Neutral\", \"Negative\", 'Irrelevant'])\n",
        "# .set_titles(\"{col_name}\")\n",
        "# .despine(left=True))  "
      ],
      "metadata": {
        "id": "fDucnE6j_P8J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RfeT0MkjjLdz"
      },
      "outputs": [],
      "source": [
        "# # Zobaczmy sobie najczciej wystpujce sowa w klasach s贸w pozytywnych, neutralnych, negatywnych i irrelevant\n",
        "\n",
        "# import wordcloud\n",
        "# from wordcloud import WordCloud\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Neutralne tweety\n",
        "# neutral_list = list(map(str, train_data [train_data['Class'] == 0]['content']))\n",
        "\n",
        "# neutral_words = \" \".join(neutral_list)\n",
        "# neutral_plot = WordCloud(width = 512, height = 512).generate(neutral_words)\n",
        "\n",
        "# # Pozytywne tweety\n",
        "# positive_list = list(map(str, train_data [train_data['Class'] == 1]['content']))\n",
        "\n",
        "# positive_words = \" \".join(positive_list)\n",
        "# positive_plot = WordCloud(width = 512, height = 512).generate(positive_words)\n",
        "\n",
        "# # Negatywne tweety\n",
        "# negative_list = list(map(str, train_data [train_data['Class'] == 2]['content']))\n",
        "\n",
        "# negative_words = \" \".join(negative_list)\n",
        "# negative_plot = WordCloud(width = 512, height = 512).generate(negative_words)\n",
        "\n",
        "# plt.figure(figsize=(10,8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Iz2VYWJ-6KzW"
      },
      "outputs": [],
      "source": [
        "# images = [neutral_plot, positive_plot, negative_plot]\n",
        "# image_names = ['neutral', 'positive', 'negative']\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(20,5))\n",
        "# plt.suptitle('Most frequent words in tweets class-wise',y = 0.95, x = 0.4, weight='heavy', size='xx-large')\n",
        "# columns = 4\n",
        "# for i, image in enumerate(images):\n",
        "#     plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "#     plt.axis('off')\n",
        "#     plt.title(image_names[i])\n",
        "#     plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rozdzielamy na dane wejciowe i wyjciowe"
      ],
      "metadata": {
        "id": "Yigl7DLp_cHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Po tym zmieniamy kom贸rki ze stringami na floaty za pomocVectorizera"
      ],
      "metadata": {
        "id": "37zZNe0A_n_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aO2ymm3i8gCy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train = train_data.drop(columns='Class')\n",
        "y_train = np.asarray(train_data['Class']).reshape(-1, 1).ravel()\n",
        "y_train = y_train.astype('int')\n",
        "\n",
        "X_test = test_data.drop(columns='Class')\n",
        "y_test = np.asarray(test_data['Class']).reshape(-1, 1).ravel()\n",
        "y_test = y_test.astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDYYgsO4HxKX"
      },
      "source": [
        "### Aby polepszywyniki klasyfikator贸w poddajemy dane stemmingowi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmRzg4o6IVyu"
      },
      "source": [
        "Jednak jako, 偶e jedna z naszych kolumn zawiera tylko nazwy gier/temat贸w o kt贸rych jest pisane, to stemmingowi chcemy poddajedynie kolumnz trecitweet贸w, bo to tam szukanie s贸w o wsp贸lnych korzeniach znaczeniowych bdzie mieznaczenie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dP3zZ0aJKl77"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "words = stopwords.words(\"english\")\n",
        "train_data['cleaned'] = train_data['content'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
        "test_data['cleaned'] = test_data['content'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnOPvA2R8q2_"
      },
      "source": [
        "Teraz jak ju偶 mamy zwizualizowane dane i po stemmingu, to przekodujmy je na jzyk zrozumiay przez maszyn, tj. wektory cech.\n",
        "\n",
        " W tym celu u偶yjemy funkcji *TfidfVectorizer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S-2GLc05AoU4"
      },
      "outputs": [],
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['cleaned']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['cleaned']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BSEn5jw--_rf"
      },
      "outputs": [],
      "source": [
        "# Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "vectorizer_text =  TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2)) # stw贸rz instancje obiektu CountVectorizer dla kodowania tekstu tweet贸w\n",
        "vectorizer_instances = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystajc ze sownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gW6D6c7Z_ul3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "285f404f-37d2-4474-d7cd-84f9b3265850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dane treningowe: n_samples: 73996, n_features: 103542\n"
          ]
        }
      ],
      "source": [
        "print(\"Dane treningowe: n_samples: %d, n_features: %d\" % X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii78Uj9REIrm"
      },
      "source": [
        "Odwrotne mapowanie cech na sowa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gPfcl7zBAMT0"
      },
      "outputs": [],
      "source": [
        "# # Odwrotne mapowanie tweet贸w na sowa\n",
        "\n",
        "# Dla kolumny z tekstem tweet贸w\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Byw6-3Kdg3k"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UelXhyVHdgUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b72f0c-d801-4f6f-a7cf-a0ee1f769d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.939\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.92      0.92       277\n",
            "           1       0.95      0.96      0.95       457\n",
            "           2       0.93      0.93      0.93       266\n",
            "\n",
            "    accuracy                           0.94      1000\n",
            "   macro avg       0.94      0.93      0.94      1000\n",
            "weighted avg       0.94      0.94      0.94      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[254  14   9]\n",
            " [ 10 438   9]\n",
            " [  9  10 247]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model_NB = MultinomialNB()\n",
        "model_NB.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_NB.predict(X_test)\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWYO9AXDgj-K"
      },
      "source": [
        "*** Inspiracja kodem z https://towardsdatascience.com/multi-class-text-classification-with-sklearn-and-nltk-in-python-a-software-engineering-use-case-779d4a28ba5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "z5z5foCldsFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c47a26-1e93-4e4f-dc54-18113e36cba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 1.000\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       277\n",
            "           1       1.00      1.00      1.00       457\n",
            "           2       1.00      1.00      1.00       266\n",
            "\n",
            "    accuracy                           1.00      1000\n",
            "   macro avg       1.00      1.00      1.00      1000\n",
            "weighted avg       1.00      1.00      1.00      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[277   0   0]\n",
            " [  0 457   0]\n",
            " [  0   0 266]]\n",
            "MCC =  1.000, Balanced ACC = 1.000  \n"
          ]
        }
      ],
      "source": [
        "# instead of doing these steps one at a time, we can use a pipeline to complete them all at once\n",
        "pipeline = Pipeline([('chi',  SelectKBest(chi2, k=1200)),\n",
        "                     ('clf', MultinomialNB())])\n",
        "\n",
        "# fitting our model and save it in a pickle for later use\n",
        "model_NB = pipeline.fit(X_train, y_train)\n",
        "with open('Multinomial.pickle', 'wb') as f:\n",
        "    pickle.dump(model_NB, f)\n",
        "y_pred = np.array(y_test)\n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNgQ5EsoftTk"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JCatqNVlhBZw"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
        "model_SVM = LinearSVC(C=1E-2, dual=False, max_iter=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td3aryyPhDcG"
      },
      "source": [
        "Do SVM ju偶musimy bezwzgldnie przeskalowanasze dane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BJHTTOCTh-fd"
      },
      "outputs": [],
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['cleaned']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['cleaned']\n",
        "\n",
        "# Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "vectorizer_text =  TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2)) # stw贸rz instancje obiektu CountVectorizer dla kodowania tekstu tweet贸w\n",
        "vectorizer_instances = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystajc ze sownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# <--- !! ---> #\n",
        "\n",
        "\n",
        "\n",
        "# # Odwrotne mapowanie tweet贸w na sowa\n",
        "\n",
        "# Dla kolumny z tekstem tweet贸w\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ejtax03Gf2Wx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc3d31c-85e9-478e-ff62-434bab594f96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=0.01, dual=False, max_iter=10000)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Fitowanie modelu (trwa ok 3 min)\n",
        "\n",
        "model_SVM.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVBxlbTPNhow",
        "outputId": "aac5f604-d934-4823-c4f3-d51d1b2a2311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.975\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97       277\n",
            "           1       0.98      0.98      0.98       457\n",
            "           2       0.97      0.97      0.97       266\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.97      0.97      0.97      1000\n",
            "weighted avg       0.98      0.97      0.97      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[267   4   6]\n",
            " [  4 450   3]\n",
            " [  2   6 258]]\n",
            "MCC =  0.961, Balanced ACC = 0.973  \n"
          ]
        }
      ],
      "source": [
        "# Walidacja \n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "y_pred = model_SVM.predict(X_test)\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muNxoVJmtvYe"
      },
      "source": [
        "Dostajemy bardzo dobry wynik, skutecznona poziomie 97%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz3j0lMrtYsA"
      },
      "source": [
        "## Regresja Logistyczna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MxDvZTa-t7Zd"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Tu znowu model liczy siok 3 min\n",
        "\n",
        "# Teraz pewnie z 20 min\n",
        "model_log = LogisticRegression(solver = 'lbfgs', max_iter=10000, C=0.1)\n",
        "\n",
        "# W wyniku tego szukania wybrano C=0.1\n",
        "\n",
        "# grid_values = {'penalty': ['l2'], 'C': [0.1,1,10]}\n",
        "# model_log = GridSearchCV(model_log, param_grid=grid_values, n_jobs=-1, verbose=4)\n",
        "\n",
        "model_log.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YxNwr1LPv54m"
      },
      "outputs": [],
      "source": [
        "# entities_train = train_data['entity']\n",
        "# contents_train = train_data['cleaned']\n",
        "\n",
        "# entities_test = test_data['entity']\n",
        "# contents_test = test_data['cleaned']\n",
        "\n",
        "# # Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "# vectorizer_text =  TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2)) # stw贸rz instancje obiektu CountVectorizer dla kodowania tekstu tweet贸w\n",
        "# vectorizer_instances = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "# entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "# contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "# X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# # Wektoryzowanie i przetransformowanie danych testowych korzystajc ze sownika stworzonego na bazie danych treningowych\n",
        "\n",
        "# entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "# contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "# X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "\n",
        "# scaler = StandardScaler(with_mean=False)\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# # <--- !! ---> #\n",
        "\n",
        "\n",
        "\n",
        "# # # Odwrotne mapowanie tweet贸w na sowa\n",
        "\n",
        "# # Dla kolumny z tekstem tweet贸w\n",
        "\n",
        "# feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "# feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# # Dla kolumny z tematami\n",
        "\n",
        "# feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "# feature_names_instances = np.asarray(feature_names_instances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "aG-o5Wuju3mY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84e9647-e1c3-418e-dfef-2ac5acd8bbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.975\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97       277\n",
            "           1       0.98      0.98      0.98       457\n",
            "           2       0.96      0.97      0.97       266\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.97      0.97      0.97      1000\n",
            "weighted avg       0.98      0.97      0.98      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[268   3   6]\n",
            " [  5 448   4]\n",
            " [  3   4 259]]\n",
            "MCC =  0.961, Balanced ACC = 0.974  \n"
          ]
        }
      ],
      "source": [
        "y_pred = model_log.predict(X_test)  \n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation\n",
        "\n",
        "scores = cross_validate(model_log, X_train, y_train, n_jobs=-1, scoring='accuracy', verbose=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnqPk0Cu94BB",
        "outputId": "8a74a70f-a1fc-48ce-a118-907c62377b1b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.1min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr3VyiT2yfwD"
      },
      "source": [
        "## DecissionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "n2UslwUeyi2W"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "\n",
        "# Deklarujemy klasyfikator\n",
        "model_tree = tree.DecisionTreeClassifier()\n",
        "# Fitujemy do danych treningowych\n",
        "model_tree = model_tree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wvSOoiM5zHOy"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import Image \n",
        "\n",
        "# from six import StringIO\n",
        "# import pydot \n",
        "\n",
        "# dot_data = StringIO()  \n",
        "# tree.export_graphviz(clf, out_file=dot_data,  \n",
        "                         # class_names=['Neutral', 'Positive', 'Negative'],  \n",
        "                         # filled=True, rounded=True,  \n",
        "                         # special_characters=True) \n",
        "# graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
        "# Image(graph[0].create_png())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_tree.predict(X_test)  \n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ],
      "metadata": {
        "id": "j-QAj8bP4Bkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb6d158-4f7f-4496-9151-456debc7a784"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.924\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91       277\n",
            "           1       0.94      0.92      0.93       457\n",
            "           2       0.89      0.95      0.92       266\n",
            "\n",
            "    accuracy                           0.92      1000\n",
            "   macro avg       0.92      0.93      0.92      1000\n",
            "weighted avg       0.92      0.92      0.92      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[249  17  11]\n",
            " [ 17 421  19]\n",
            " [  4   8 254]]\n",
            "MCC =  0.883, Balanced ACC = 0.925  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Elapsed time\\n--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "RwRVasC-4JW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fe08da-403e-4bb3-d94a-6025c46ed967"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time\n",
            "--- 488.72840332984924 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bez CV\n",
        "\n",
        "Elapsed time\n",
        "--- 472.361172914505 seconds ---"
      ],
      "metadata": {
        "id": "VzuVKqZX9rcf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Cybiski_02_zadanie_domowe.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}