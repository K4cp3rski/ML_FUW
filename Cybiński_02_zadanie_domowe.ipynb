{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K4cp3rski/ML_FUW/blob/master/Cybi%C5%84ski_02_zadanie_domowe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYtv4ZSbTx5X"
      },
      "source": [
        "# Praca domowa I, zadanie II\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaX60EphVC74"
      },
      "source": [
        "## Tre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4_wGX3_coFa"
      },
      "source": [
        "### Wstp fabularny"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJlu3kmQchcE"
      },
      "source": [
        "Wyobra藕 sobie, 偶e jeste pracownikiem w firmie sprzedajcej kompleksow usug tworzenia wizerunk贸w medialnych. Oddzia, w kt贸rym pracujesz obsuguje wa偶nego klienta dziaajcego w bran偶y gier i usug cyfrowych.\n",
        "\n",
        "Twoim zadaniem jest przygotowa model uczenia maszynowego, kt贸ry okrela bdzie nastawienie emocjonalne post贸w z Twittera. Zesp贸 odpowiadajcy za zbieranie danych wanie dostarczy zestaw danych dla Ciebie.\n",
        "\n",
        "Do tej pory klasyfikowaniem nastroj贸w z twitt贸w zajmowa si zesp贸 ekspert贸w. Rozwizanie takie jest bardzo wolne i drogie, a dokadno ekspert贸w wynosi tylko 95%. Dlatego zarzd firmy zleci wdro偶enie modelu uczenia maszynowego.\n",
        "\n",
        "Tw贸j model stanowi bdzie jedynie cz wikszego produktu oferowanego przez Twoj firm. Wyniki Twojego modelu bd bezporednio wykorzystywane przez nastpny zesp贸, kt贸rego zadaniem jest przygotowa kolejny model uczenia maszynowego przewidujcy reakcje opinii publicznej na posty klienta.\n",
        "\n",
        "Prace zespou, kt贸ry korzysta bdzie z Twojego modelu s ju偶 bardzo zaawansowane, dlatego nie mo偶e on pozwoli sobie na 偶adne dodatkowe zmiany w swoim projekcie. Absolutnie konieczne jest, aby Tw贸j model przyporzdkowywa posty do jednej z trzech klas 'Positive', 'Negative', 'Neutral' lub analogicznych. Posty nie na temat powinny by klasyfikowane jako 'Neutral'.\n",
        "\n",
        "Notebook z Twoim projektem bdzie oglda Tw贸j szef, wic koniecznie zadbaj, 偶eby znalazy si w nim najwa偶niejsze przemylenia, a rysunki byy adne.\n",
        "\n",
        "Powodzenia "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRk5odT-VpHl"
      },
      "source": [
        "### Polecenia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqep_SopVL6B"
      },
      "source": [
        "1. Wstpna obr贸bka danych:\n",
        "\n",
        " - zaaduj zbi贸r treningowy i testowy,\n",
        " - usu wiersze o brakujcych elementach,\n",
        " - w kolumnie `sentiment` zamie wartoci `'Irrelevant'` na `'Neutral'`.\n",
        "\n",
        "1. Wykonaj wizualizacje danych:\n",
        "\n",
        " - histogram temat贸w twitt贸w (`entity`),\n",
        " - histogram nastawie (`sentiment`),\n",
        " - najczciej padajcych s贸w w treci twitt贸w (`content`).\n",
        "\n",
        "1. Przygotuj dane:\n",
        "\n",
        " - przygotuj zbi贸r cech poprzez wektoryzacje kolumny `content`, \n",
        " - przygotuj etykiety poprzez zakodowanie tekstowych wartoci w kolumnie `sentiment` do postaci liczbowej.\n",
        "\n",
        "  Nastpnie wytrenuj naiwny model bayesowski. Sprawd藕 dziaanie modelu na kilku wasnorcznie napisanych wiadomociach. \n",
        "\n",
        "1. Wytrenuj modele:\n",
        " - naiwny bayesowski,\n",
        " - liniowy SVM,\n",
        " - regresji logistycznej,\n",
        " - drzewo decyzyjne.\n",
        "\n",
        "  Sprawd藕 model na danych treningowych (walidacja krzy偶owa) i testowych, nastpnie wybierz najlepszy model. Uzasadnij sw贸j wyb贸r.\n",
        "  \n",
        "1. Zesp贸 ekspert贸w rcznie klasyfikuje dane z dokadnoci 95%. Por贸wnaj z nimi sw贸j model i napisz jakie s przewagi Twojego modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZpaDghkU_B7"
      },
      "source": [
        "### Zbi贸r danych\n",
        "\n",
        "Zbi贸r danych zosta przygotowany na podstawie zbioru [Twitter Sentiment Analysis](https://www.kaggle.com/jp797498e/twitter-entity-sentiment-analysis) i skada si z dw贸ch plik贸w:\n",
        "-  `twitter_training.csv` - zbi贸r treningowy,\n",
        "- `twitter_validation.csv` - zbi贸r testowy.\n",
        "\n",
        "Archiwum z plikami mo偶na pobra z [dysku google](https://drive.google.com/file/d/1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB/view?usp=sharing) lub odkomentowujc poni偶sze linie:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "lrlti4Zk4Lhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlvbsKdVU0hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a490b45-ff58-41ea-87af-d666b9d66aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB\n",
            "To: /content/twitter.zip\n",
            "\r  0% 0.00/2.07M [00:00<?, ?B/s]\r100% 2.07M/2.07M [00:00<00:00, 133MB/s]\n",
            "Archive:  twitter.zip\n",
            "  inflating: twitter_training.csv    \n",
            "  inflating: twitter_validation.csv  \n"
          ]
        }
      ],
      "source": [
        "# ! pip install gdown\n",
        "! gdown https://drive.google.com/uc?id=1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB\n",
        "! unzip twitter.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95mTOShAjGji"
      },
      "outputs": [],
      "source": [
        "# !pip install wordcloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGrsOK5pT1Xw"
      },
      "source": [
        "# Rozwizanie"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impoty \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TERrt4LEZdp",
        "outputId": "df9a7605-bd65-444b-fd13-00b7592cc7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adowanie danych"
      ],
      "metadata": {
        "id": "de84o_Yt-3LL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ0m63_gU41z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3b79e8-f07c-4f39-beef-cae89a5a7e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zainstalowana wersja scikit-learn: 1.0.1.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "print('Zainstalowana wersja scikit-learn: {}.'.format(sklearn.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10) # aby wykresy w Colabie byy wiksze\n",
        "\n",
        "import numpy as np\n",
        "from scipy import diag, interp\n",
        "from itertools import cycle\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6Ykp7XDRlF5"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('twitter_training.csv')\n",
        "test_data = pd.read_csv('twitter_validation.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7rWWbBA9wTr"
      },
      "source": [
        "Jak widzimy globalnie (Obrazek 1) klasy s w miare zbalansowane, liczebno nie r贸偶ni siwicej jak 2-krotnie. W zwizku z tym wydaje si, 偶e nie ma potrzeby sztucznego wyr贸wnywania ich liczebnoci i mo偶na przej do dalszego etapu preprocessingu danych"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing danych"
      ],
      "metadata": {
        "id": "uNbac6Dd_Do3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zmieniamy labelki na integerowe"
      ],
      "metadata": {
        "id": "FYYpHtacAvIs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMIHVxWiR4s8"
      },
      "outputs": [],
      "source": [
        "# Przydzielamy klasom integerowe labelki\n",
        "\n",
        "# Neutral = Class 0\n",
        "train_data.loc[train_data['sentiment'] == 'Positive', 'sentiment'] = 0\n",
        "# Positive = Class 1\n",
        "train_data.loc[train_data['sentiment'] == 'Neutral', 'sentiment'] = 1\n",
        "# Negative = Class 2\n",
        "train_data.loc[train_data['sentiment'] == 'Negative', 'sentiment'] = 2\n",
        "# Irrelevant = Class 1\n",
        "train_data.loc[train_data['sentiment'] == 'Irrelevant', 'sentiment'] = 1\n",
        "\n",
        "# A teraz to samo dla zbiotu testowego\n",
        "\n",
        "# Neutral = Class 0\n",
        "test_data.loc[test_data['sentiment'] == 'Positive', 'sentiment'] = 0\n",
        "# Positive = Class 1\n",
        "test_data.loc[test_data['sentiment'] == 'Neutral', 'sentiment'] = 1\n",
        "# Negative = Class 2\n",
        "test_data.loc[test_data['sentiment'] == 'Negative', 'sentiment'] = 2\n",
        "# Irrelevant = Class 1\n",
        "test_data.loc[test_data['sentiment'] == 'Irrelevant', 'sentiment'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYH-7SsoXZy1"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.drop(columns='id')\n",
        "test_data = test_data.drop(columns='id')\n",
        "\n",
        "train_data = train_data.rename(columns={'sentiment':'Class'})\n",
        "test_data = test_data.rename(columns={'sentiment':'Class'})\n",
        "\n",
        "# Pozbywamy si wierszy z niepenymi informacjani (NaN)\n",
        "train_data = train_data.dropna(axis='rows', how='all', thresh=int(train_data.shape[1]))\n",
        "test_data = test_data.dropna(axis='rows', how='all', thresh=int(test_data.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wizualizacja danych, oraz najczciej wystpujcych s贸w w tweetach danych kategorii"
      ],
      "metadata": {
        "id": "dkfna3Sm_SYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Wizualizacja danych\n",
        "\n",
        "# f = sns.countplot(x='sentiment', data=train_data)\n",
        "# h = sns.catplot(y='entity', hue='sentiment', data=train_data, kind='count', height=20, aspect=1)\n",
        "# g = sns.catplot(x='sentiment', col='entity' , col_wrap=4, kind='count', data=train_data, height=4.5, aspect=1.2)\n",
        "# i = sns.catplot(y='entity', data=train_data, height=20, kind='count')\n",
        "\n",
        "# (g.set_axis_labels(\"\", \"Tweet count\")\n",
        "# .set_xticklabels([\"Positive\", \"Neutral\", \"Negative\", 'Irrelevant'])\n",
        "# .set_titles(\"{col_name}\")\n",
        "# .despine(left=True))  "
      ],
      "metadata": {
        "id": "fDucnE6j_P8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfeT0MkjjLdz"
      },
      "outputs": [],
      "source": [
        "# # Zobaczmy sobie najczciej wystpujce sowa w klasach s贸w pozytywnych, neutralnych, negatywnych i irrelevant\n",
        "\n",
        "# import wordcloud\n",
        "# from wordcloud import WordCloud\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Neutralne tweety\n",
        "# neutral_list = list(map(str, train_data [train_data['Class'] == 0]['content']))\n",
        "\n",
        "# neutral_words = \" \".join(neutral_list)\n",
        "# neutral_plot = WordCloud(width = 512, height = 512).generate(neutral_words)\n",
        "\n",
        "# # Pozytywne tweety\n",
        "# positive_list = list(map(str, train_data [train_data['Class'] == 1]['content']))\n",
        "\n",
        "# positive_words = \" \".join(positive_list)\n",
        "# positive_plot = WordCloud(width = 512, height = 512).generate(positive_words)\n",
        "\n",
        "# # Negatywne tweety\n",
        "# negative_list = list(map(str, train_data [train_data['Class'] == 2]['content']))\n",
        "\n",
        "# negative_words = \" \".join(negative_list)\n",
        "# negative_plot = WordCloud(width = 512, height = 512).generate(negative_words)\n",
        "\n",
        "# plt.figure(figsize=(10,8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz2VYWJ-6KzW"
      },
      "outputs": [],
      "source": [
        "# images = [neutral_plot, positive_plot, negative_plot]\n",
        "# image_names = ['neutral', 'positive', 'negative']\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(20,5))\n",
        "# plt.suptitle('Most frequent words in tweets class-wise',y = 0.95, x = 0.4, weight='heavy', size='xx-large')\n",
        "# columns = 4\n",
        "# for i, image in enumerate(images):\n",
        "#     plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "#     plt.axis('off')\n",
        "#     plt.title(image_names[i])\n",
        "#     plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rozdzielamy na dane wejciowe i wyjciowe"
      ],
      "metadata": {
        "id": "Yigl7DLp_cHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Po tym zmieniamy kom贸rki ze stringami na floaty za pomocVectorizera"
      ],
      "metadata": {
        "id": "37zZNe0A_n_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO2ymm3i8gCy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train = train_data.drop(columns='Class')\n",
        "y_train = np.asarray(train_data['Class']).reshape(-1, 1).ravel()\n",
        "y_train = y_train.astype('int')\n",
        "\n",
        "X_test = test_data.drop(columns='Class')\n",
        "y_test = np.asarray(test_data['Class']).reshape(-1, 1).ravel()\n",
        "y_test = y_test.astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnOPvA2R8q2_"
      },
      "source": [
        "Teraz jak ju偶 mamy zwizualizowane dane, to przekodujmy je na jzyk zrozumiay przez maszyn, tj. wektory cech.\n",
        "\n",
        " W tym celu u偶yjemy funkcji *TfidfVectorizer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-2GLc05AoU4"
      },
      "outputs": [],
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['content']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSEn5jw--_rf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Dopasowanie i wektoryzowanie dla danych treningowych\n",
        "\n",
        "vectorizer_text = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania tekstu tweet贸w\n",
        "vectorizer_instances = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystajc ze sownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW6D6c7Z_ul3",
        "outputId": "7d79af1f-5e8e-4bee-f619-2484179edf9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dane treningowe: n_samples: 73996, n_features: 31100\n"
          ]
        }
      ],
      "source": [
        "print(\"Dane treningowe: n_samples: %d, n_features: %d\" % X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii78Uj9REIrm"
      },
      "source": [
        "Odwrotne mapowanie cech na sowa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPfcl7zBAMT0"
      },
      "outputs": [],
      "source": [
        "# Dla kolumny z tekstem tweet贸w\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDYYgsO4HxKX"
      },
      "source": [
        "### Aby polepszywyniki klasyfikator贸w poddajemy dane stemmingowi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmRzg4o6IVyu"
      },
      "source": [
        "Jednak jako, 偶e jedna z naszych kolumn zawiera tylko nazwy gier/temat贸w o kt贸rych jest pisane, to stemmingowi chcemy poddajedynie kolumnz trecitweet贸w, bo to tam szukanie s贸w o wsp贸lnych korzeniach znaczeniowych bdzie mieznaczenie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dP3zZ0aJKl77"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "words = stopwords.words(\"english\")\n",
        "train_data['cleaned'] = train_data['content'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
        "test_data['cleaned'] = test_data['content'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Byw6-3Kdg3k"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqwF1_TlIIZj"
      },
      "outputs": [],
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['cleaned']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['cleaned']\n",
        "\n",
        "# Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "vectorizer_text =  TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2)) # stw贸rz instancje obiektu CountVectorizer dla kodowania tekstu tweet贸w\n",
        "vectorizer_instances = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystajc ze sownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "\n",
        "\n",
        "# <--- !! ---> #\n",
        "\n",
        "\n",
        "\n",
        "# # Odwrotne mapowanie tweet贸w na sowa\n",
        "\n",
        "# Dla kolumny z tekstem tweet贸w\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UelXhyVHdgUg",
        "outputId": "4a72e48d-d73d-4f26-d0bf-a7939c826b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.939\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.92      0.92       277\n",
            "           1       0.95      0.96      0.95       457\n",
            "           2       0.93      0.93      0.93       266\n",
            "\n",
            "    accuracy                           0.94      1000\n",
            "   macro avg       0.94      0.93      0.94      1000\n",
            "weighted avg       0.94      0.94      0.94      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[254  14   9]\n",
            " [ 10 438   9]\n",
            " [  9  10 247]]\n"
          ]
        }
      ],
      "source": [
        "model_NB = MultinomialNB()\n",
        "model_NB.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_NB.predict(X_test)\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWYO9AXDgj-K"
      },
      "source": [
        "*** Inspiracja kodem z https://towardsdatascience.com/multi-class-text-classification-with-sklearn-and-nltk-in-python-a-software-engineering-use-case-779d4a28ba5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5z5foCldsFd",
        "outputId": "88b3821b-1517-4a21-b3ff-53d8f1adecf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 1.000\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       277\n",
            "           1       1.00      1.00      1.00       457\n",
            "           2       1.00      1.00      1.00       266\n",
            "\n",
            "    accuracy                           1.00      1000\n",
            "   macro avg       1.00      1.00      1.00      1000\n",
            "weighted avg       1.00      1.00      1.00      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[277   0   0]\n",
            " [  0 457   0]\n",
            " [  0   0 266]]\n",
            "MCC =  1.000, Balanced ACC = 1.000  \n"
          ]
        }
      ],
      "source": [
        "# instead of doing these steps one at a time, we can use a pipeline to complete them all at once\n",
        "pipeline = Pipeline([('chi',  SelectKBest(chi2, k=1200)),\n",
        "                     ('clf', MultinomialNB())])\n",
        "\n",
        "# fitting our model and save it in a pickle for later use\n",
        "model_NB = pipeline.fit(X_train, y_train)\n",
        "with open('Multinomial.pickle', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "y_pred = np.array(y_test)\n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNgQ5EsoftTk"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCatqNVlhBZw"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
        "model_SVM = LinearSVC(C=1E-2, dual=False, max_iter=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td3aryyPhDcG"
      },
      "source": [
        "Do SVM ju偶musimy bezwzgldnie przeskalowanasze dane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJHTTOCTh-fd"
      },
      "outputs": [],
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['cleaned']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['cleaned']\n",
        "\n",
        "# Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "vectorizer_text =  TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2)) # stw贸rz instancje obiektu CountVectorizer dla kodowania tekstu tweet贸w\n",
        "vectorizer_instances = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystajc ze sownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# <--- !! ---> #\n",
        "\n",
        "\n",
        "\n",
        "# # Odwrotne mapowanie tweet贸w na sowa\n",
        "\n",
        "# Dla kolumny z tekstem tweet贸w\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejtax03Gf2Wx",
        "outputId": "73268bbb-4648-490b-e3ee-69be40b4be9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=0.01, dual=False, max_iter=10000)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "# Fitowanie modelu (trwa ok 3 min)\n",
        "model_SVM.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVBxlbTPNhow",
        "outputId": "8e3beba1-7907-4bbb-8947-2a7d5ad2018a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.975\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97       277\n",
            "           1       0.98      0.98      0.98       457\n",
            "           2       0.97      0.97      0.97       266\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.97      0.97      0.97      1000\n",
            "weighted avg       0.98      0.97      0.97      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[267   4   6]\n",
            " [  4 450   3]\n",
            " [  2   6 258]]\n",
            "MCC =  0.961, Balanced ACC = 0.973  \n"
          ]
        }
      ],
      "source": [
        "# Walidacja \n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "y_pred = model_SVM.predict(X_test)\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muNxoVJmtvYe"
      },
      "source": [
        "Dostajemy bardzo dobry wynik, skutecznona poziomie 97%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz3j0lMrtYsA"
      },
      "source": [
        "## Regresja Logistyczna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxDvZTa-t7Zd",
        "outputId": "c668173c-65ca-451d-a5f9-975bfff5bbf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=10000)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Tu znowu model liczy siok 3 min\n",
        "model_log = LogisticRegression(solver = 'lbfgs', max_iter=10000)\n",
        "model_log.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxNwr1LPv54m"
      },
      "outputs": [],
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['cleaned']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['cleaned']\n",
        "\n",
        "# Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "vectorizer_text =  TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2)) # stw贸rz instancje obiektu CountVectorizer dla kodowania tekstu tweet贸w\n",
        "vectorizer_instances = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystajc ze sownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# <--- !! ---> #\n",
        "\n",
        "\n",
        "\n",
        "# # Odwrotne mapowanie tweet贸w na sowa\n",
        "\n",
        "# Dla kolumny z tekstem tweet贸w\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG-o5Wuju3mY",
        "outputId": "0498ad54-5abc-4e81-b267-2d10ffc4aa05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.974\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.97       277\n",
            "           1       0.98      0.98      0.98       457\n",
            "           2       0.96      0.97      0.97       266\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.97      0.97      0.97      1000\n",
            "weighted avg       0.97      0.97      0.97      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[267   4   6]\n",
            " [  5 448   4]\n",
            " [  2   5 259]]\n",
            "MCC =  0.960, Balanced ACC = 0.973  \n"
          ]
        }
      ],
      "source": [
        "y_pred = model_log.predict(X_test)  \n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr3VyiT2yfwD"
      },
      "source": [
        "## DecissionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2UslwUeyi2W"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "\n",
        "# Deklarujemy klasyfikator\n",
        "model_tree = tree.DecisionTreeClassifier()\n",
        "# Fitujemy do danych treningowych\n",
        "model_tree = model_tree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvSOoiM5zHOy"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import Image \n",
        "\n",
        "# from six import StringIO\n",
        "# import pydot \n",
        "\n",
        "# dot_data = StringIO()  \n",
        "# tree.export_graphviz(clf, out_file=dot_data,  \n",
        "                         # class_names=['Neutral', 'Positive', 'Negative'],  \n",
        "                         # filled=True, rounded=True,  \n",
        "                         # special_characters=True) \n",
        "# graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
        "# Image(graph[0].create_png())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_tree.predict(X_test)  \n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-QAj8bP4Bkz",
        "outputId": "cdc2ec79-d8d8-4822-bf71-5d21ca06e74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.923\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91       277\n",
            "           1       0.94      0.92      0.93       457\n",
            "           2       0.90      0.95      0.93       266\n",
            "\n",
            "    accuracy                           0.92      1000\n",
            "   macro avg       0.92      0.92      0.92      1000\n",
            "weighted avg       0.92      0.92      0.92      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[250  19   8]\n",
            " [ 18 420  19]\n",
            " [  5   8 253]]\n",
            "MCC =  0.881, Balanced ACC = 0.924  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Elapsed time\\n--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "RwRVasC-4JW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bez CV\n",
        "\n",
        "Elapsed time\n",
        "--- 472.361172914505 seconds ---"
      ],
      "metadata": {
        "id": "VzuVKqZX9rcf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Cybiski_02_zadanie_domowe.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}