{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cybiski_02_zadanie_domowe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K4cp3rski/ML_FUW/blob/master/Cybi%C5%84ski_02_zadanie_domowe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Praca domowa I, zadanie II\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sYtv4ZSbTx5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tre"
      ],
      "metadata": {
        "id": "NaX60EphVC74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wstp fabularny"
      ],
      "metadata": {
        "id": "D4_wGX3_coFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wyobra藕 sobie, 偶e jeste pracownikiem w firmie sprzedajcej kompleksow usug tworzenia wizerunk贸w medialnych. Oddzia, w kt贸rym pracujesz obsuguje wa偶nego klienta dziaajcego w bran偶y gier i usug cyfrowych.\n",
        "\n",
        "Twoim zadaniem jest przygotowa model uczenia maszynowego, kt贸ry okrela bdzie nastawienie emocjonalne post贸w z Twittera. Zesp贸 odpowiadajcy za zbieranie danych wanie dostarczy zestaw danych dla Ciebie.\n",
        "\n",
        "Do tej pory klasyfikowaniem nastroj贸w z twitt贸w zajmowa si zesp贸 ekspert贸w. Rozwizanie takie jest bardzo wolne i drogie, a dokadno ekspert贸w wynosi tylko 95%. Dlatego zarzd firmy zleci wdro偶enie modelu uczenia maszynowego.\n",
        "\n",
        "Tw贸j model stanowi bdzie jedynie cz wikszego produktu oferowanego przez Twoj firm. Wyniki Twojego modelu bd bezporednio wykorzystywane przez nastpny zesp贸, kt贸rego zadaniem jest przygotowa kolejny model uczenia maszynowego przewidujcy reakcje opinii publicznej na posty klienta.\n",
        "\n",
        "Prace zespou, kt贸ry korzysta bdzie z Twojego modelu s ju偶 bardzo zaawansowane, dlatego nie mo偶e on pozwoli sobie na 偶adne dodatkowe zmiany w swoim projekcie. Absolutnie konieczne jest, aby Tw贸j model przyporzdkowywa posty do jednej z trzech klas 'Positive', 'Negative', 'Neutral' lub analogicznych. Posty nie na temat powinny by klasyfikowane jako 'Neutral'.\n",
        "\n",
        "Notebook z Twoim projektem bdzie oglda Tw贸j szef, wic koniecznie zadbaj, 偶eby znalazy si w nim najwa偶niejsze przemylenia, a rysunki byy adne.\n",
        "\n",
        "Powodzenia "
      ],
      "metadata": {
        "id": "rJlu3kmQchcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polecenia"
      ],
      "metadata": {
        "id": "GRk5odT-VpHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Wstpna obr贸bka danych:\n",
        "\n",
        " - zaaduj zbi贸r treningowy i testowy,\n",
        " - usu wiersze o brakujcych elementach,\n",
        " - w kolumnie `sentiment` zamie wartoci `'Irrelevant'` na `'Neutral'`.\n",
        "\n",
        "1. Wykonaj wizualizacje danych:\n",
        "\n",
        " - histogram temat贸w twitt贸w (`entity`),\n",
        " - histogram nastawie (`sentiment`),\n",
        " - najczciej padajcych s贸w w treci twitt贸w (`content`).\n",
        "\n",
        "1. Przygotuj dane:\n",
        "\n",
        " - przygotuj zbi贸r cech poprzez wektoryzacje kolumny `content`, \n",
        " - przygotuj etykiety poprzez zakodowanie tekstowych wartoci w kolumnie `sentiment` do postaci liczbowej.\n",
        "\n",
        "  Nastpnie wytrenuj naiwny model bayesowski. Sprawd藕 dziaanie modelu na kilku wasnorcznie napisanych wiadomociach. \n",
        "\n",
        "1. Wytrenuj modele:\n",
        " - naiwny bayesowski,\n",
        " - liniowy SVM,\n",
        " - regresji logistycznej,\n",
        " - drzewo decyzyjne.\n",
        "\n",
        "  Sprawd藕 model na danych treningowych (walidacja krzy偶owa) i testowych, nastpnie wybierz najlepszy model. Uzasadnij sw贸j wyb贸r.\n",
        "  \n",
        "1. Zesp贸 ekspert贸w rcznie klasyfikuje dane z dokadnoci 95%. Por贸wnaj z nimi sw贸j model i napisz jakie s przewagi Twojego modelu."
      ],
      "metadata": {
        "id": "Iqep_SopVL6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zbi贸r danych\n",
        "\n",
        "Zbi贸r danych zosta przygotowany na podstawie zbioru [Twitter Sentiment Analysis](https://www.kaggle.com/jp797498e/twitter-entity-sentiment-analysis) i skada si z dw贸ch plik贸w:\n",
        "-  `twitter_training.csv` - zbi贸r treningowy,\n",
        "- `twitter_validation.csv` - zbi贸r testowy.\n",
        "\n",
        "Archiwum z plikami mo偶na pobra z [dysku google](https://drive.google.com/file/d/1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB/view?usp=sharing) lub odkomentowujc poni偶sze linie:"
      ],
      "metadata": {
        "id": "tZpaDghkU_B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install gdown\n",
        "# ! gdown https://drive.google.com/uc?id=1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB\n",
        "# ! unzip twitter.zip"
      ],
      "metadata": {
        "id": "VlvbsKdVU0hz"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wordcloud"
      ],
      "metadata": {
        "id": "95mTOShAjGji"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rozwizanie"
      ],
      "metadata": {
        "id": "dGrsOK5pT1Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "print('Zainstalowana wersja scikit-learn: {}.'.format(sklearn.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10) # aby wykresy w Colabie byy wiksze\n",
        "\n",
        "import numpy as np\n",
        "from scipy import diag, interp\n",
        "from itertools import cycle\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "lQ0m63_gU41z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab65370d-3b47-480e-ef40-932271871dc4"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zainstalowana wersja scikit-learn: 1.0.1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('twitter_training.csv')\n",
        "test_data = pd.read_csv('twitter_validation.csv')"
      ],
      "metadata": {
        "id": "h6Ykp7XDRlF5"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Wizualizacja danych\n",
        "\n",
        "# f = sns.countplot(x='sentiment', data=train_data)\n",
        "# h = sns.catplot(y='entity', hue='sentiment', data=train_data, kind='count', height=20, aspect=1)\n",
        "# g = sns.catplot(x='sentiment', col='entity' , col_wrap=4, kind='count', data=train_data, height=4.5, aspect=1.2)\n",
        "\n",
        "# (g.set_axis_labels(\"\", \"Tweet count\")\n",
        "# .set_xticklabels([\"Positive\", \"Neutral\", \"Negative\", 'Irrelevant'])\n",
        "# .set_titles(\"{col_name}\")\n",
        "# .despine(left=True))  "
      ],
      "metadata": {
        "id": "CIIK8rCWZT62"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jak widzimy globalnie (Obrazek 1) klasy s w miare zbalansowane, liczebno nie r贸偶ni siwicej jak 2-krotnie. W zwizku z tym wydaje si, 偶e nie ma potrzeby sztucznego wyr贸wnywania ich liczebnoci i mo偶na przej do dalszego etapu preprocessingu danych"
      ],
      "metadata": {
        "id": "h7rWWbBA9wTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Przydzielamy klasom integerowe labelki\n",
        "\n",
        "# Neutral = Class 0\n",
        "train_data.loc[train_data['sentiment'] == 'Positive', 'sentiment'] = 0\n",
        "# Positive = Class 1\n",
        "train_data.loc[train_data['sentiment'] == 'Neutral', 'sentiment'] = 1\n",
        "# Negative = Class 2\n",
        "train_data.loc[train_data['sentiment'] == 'Negative', 'sentiment'] = 2\n",
        "# Irrelevant = Class 0\n",
        "train_data.loc[train_data['sentiment'] == 'Irrelevant', 'sentiment'] = 0\n",
        "\n",
        "# A teraz to samo dla zbiotu testowego\n",
        "\n",
        "# Neutral = Class 0\n",
        "test_data.loc[test_data['sentiment'] == 'Positive', 'sentiment'] = 0\n",
        "# Positive = Class 1\n",
        "test_data.loc[test_data['sentiment'] == 'Neutral', 'sentiment'] = 1\n",
        "# Negative = Class 2\n",
        "test_data.loc[test_data['sentiment'] == 'Negative', 'sentiment'] = 2\n",
        "# Irrelevant = Class 0\n",
        "test_data.loc[test_data['sentiment'] == 'Irrelevant', 'sentiment'] = 0"
      ],
      "metadata": {
        "id": "aMIHVxWiR4s8"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_data['sentiment'].unique())\n",
        "# print(test_data['sentiment'].unique())"
      ],
      "metadata": {
        "id": "yzBl74otT0E_"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.drop(columns='id')\n",
        "test_data = test_data.drop(columns='id')\n",
        "\n",
        "train_data = train_data.rename(columns={'sentiment':'Class'})\n",
        "test_data = test_data.rename(columns={'sentiment':'Class'})\n",
        "\n",
        "# Pozbywamy si wierszy z niepenymi informacjani (NaN)\n",
        "train_data = train_data.dropna(axis='rows', how='all', thresh=int(train_data.shape[1]))\n",
        "test_data = test_data.dropna(axis='rows', how='all', thresh=int(test_data.shape[1]))"
      ],
      "metadata": {
        "id": "vYH-7SsoXZy1"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Zobaczmy sobie najczciej wystpujce sowa w klasach s贸w pozytywnych, neutralnych, negatywnych i irrelevant\n",
        "\n",
        "# import wordcloud\n",
        "# from wordcloud import WordCloud\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Neutralne tweety\n",
        "# neutral_list = list(map(str, train_data [train_data['Class'] == 0]['content']))\n",
        "\n",
        "# neutral_words = \" \".join(neutral_list)\n",
        "# neutral_plot = WordCloud(width = 512, height = 512).generate(neutral_words)\n",
        "\n",
        "# # Pozytywne tweety\n",
        "# positive_list = list(map(str, train_data [train_data['Class'] == 1]['content']))\n",
        "\n",
        "# positive_words = \" \".join(positive_list)\n",
        "# positive_plot = WordCloud(width = 512, height = 512).generate(positive_words)\n",
        "\n",
        "# # Negatywne tweety\n",
        "# negative_list = list(map(str, train_data [train_data['Class'] == 2]['content']))\n",
        "\n",
        "# negative_words = \" \".join(negative_list)\n",
        "# negative_plot = WordCloud(width = 512, height = 512).generate(negative_words)\n",
        "\n",
        "# plt.figure(figsize=(10,8))"
      ],
      "metadata": {
        "id": "RfeT0MkjjLdz"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images = [neutral_plot, positive_plot, negative_plot]\n",
        "# image_names = ['neutral', 'positive', 'negative']\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(20,5))\n",
        "# plt.suptitle('Most frequent words in tweets class-wise',y = 0.95, x = 0.4, weight='heavy', size='xx-large')\n",
        "# columns = 4\n",
        "# for i, image in enumerate(images):\n",
        "#     plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "#     plt.axis('off')\n",
        "#     plt.title(image_names[i])\n",
        "#     plt.imshow(image)"
      ],
      "metadata": {
        "id": "Iz2VYWJ-6KzW"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train = train_data.drop(columns='Class')\n",
        "y_train = np.asarray(train_data['Class']).reshape(-1, 1).ravel()\n",
        "y_train = y_train.astype('int')\n",
        "\n",
        "X_test = test_data.drop(columns='Class')\n",
        "y_test = np.asarray(test_data['Class']).reshape(-1, 1).ravel()\n",
        "y_test = y_test.astype('int')"
      ],
      "metadata": {
        "id": "aO2ymm3i8gCy"
      },
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teraz jak ju偶 mamy zwizualizowane dane, to przekodujmy je na jzyk zrozumiay przez maszyn, tj. wektory cech.\n",
        "\n",
        " W tym celu u偶yjemy funkcji *CountVectorizer*"
      ],
      "metadata": {
        "id": "lnOPvA2R8q2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['content']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['content']"
      ],
      "metadata": {
        "id": "S-2GLc05AoU4"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Dopasowanie i wektoryzowanie dla danych treningowych\n",
        "\n",
        "vectorizer_text = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania tekstu tweet贸w\n",
        "vectorizer_instances = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystajc ze sownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n"
      ],
      "metadata": {
        "id": "BSEn5jw--_rf"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dane treningowe: n_samples: %d, n_features: %d\" % X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW6D6c7Z_ul3",
        "outputId": "9cf9339e-b898-496a-81e7-c87f81c8d298"
      },
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dane treningowe: n_samples: 73996, n_features: 31100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Odwrotne mapowanie cech na sowa"
      ],
      "metadata": {
        "id": "Ii78Uj9REIrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dla kolumny z tekstem tweet贸w\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)"
      ],
      "metadata": {
        "id": "gPfcl7zBAMT0"
      },
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Tworzymy instancjklasyfikatora MultinomialNB"
      ],
      "metadata": {
        "id": "IoZt2KwdE-41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfkvbSoAE9jD",
        "outputId": "e6256042-6048-4740-f525-f92aff67231f"
      },
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 349
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Sprawd藕my na zbiorze testowym\n",
        "\n",
        "Robimy predykcj dla X_test"
      ],
      "metadata": {
        "id": "Aj7mEw2UGIIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test) # obliczamy predykcj dla tekst贸w ze zbioru testowego"
      ],
      "metadata": {
        "id": "dVKXffxlGPMk"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nPrecyzja wyliczona built-in method = {}\".format(clf.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2oD875zGjSV",
        "outputId": "3cd78cf7-0d6e-47f2-cc31-2cbb2dc54077"
      },
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.831\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       449\n",
            "           1       0.88      0.74      0.80       285\n",
            "           2       0.79      0.82      0.81       266\n",
            "\n",
            "    accuracy                           0.83      1000\n",
            "   macro avg       0.83      0.82      0.82      1000\n",
            "weighted avg       0.83      0.83      0.83      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[402  17  30]\n",
            " [ 48 210  27]\n",
            " [ 34  13 219]]\n",
            "\n",
            "Precyzja wyliczona built-in method = 0.831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mo偶emy nasz klasyfikator ulepszy, w tym celu trzeba poddadane stemmingowi"
      ],
      "metadata": {
        "id": "HDYYgsO4HxKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jednak jako, 偶e jedna z naszych kolumn zawiera tylko nazwy gier/temat贸w o kt贸rych jest pisane, to stemmingowi chcemy poddajedynie kolumnz trecitweet贸w, bo to tam szukanie s贸w o wsp贸lnych korzeniach znaczeniowych bdzie mieznaczenie"
      ],
      "metadata": {
        "id": "jmRzg4o6IVyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importy niezbdnych rzeczy\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2Vs0tpuH4R5",
        "outputId": "f3cb937e-b8d0-46c4-878c-d16dff60a70f"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))"
      ],
      "metadata": {
        "id": "dP3zZ0aJKl77"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['content']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['content']\n",
        "\n",
        "# Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "vectorizer_text = CountVectorizer(analyzer=stemmed_words) # stw贸rz instancje obiektu CountVectorizer dla kodowania tekstu tweet贸w\n",
        "vectorizer_instances = CountVectorizer() # stw贸rz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystajc ze sownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer sownika i przetransformuj dane uczce (kolumna treci)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "\n",
        "\n",
        "# <--- !! ---> #\n",
        "\n",
        "\n",
        "\n",
        "# # Odwrotne mapowanie tweet贸w na sowa\n",
        "\n",
        "# Dla kolumny z tekstem tweet贸w\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)\n",
        "\n",
        "\n",
        "\n",
        "# <--- !! ---> #\n",
        "\n",
        "\n",
        "\n",
        "# # # # Klasyfikator\n",
        "\n",
        "# clf = MultinomialNB()\n",
        "# clf.fit(X_train, y_train)\n",
        "\n",
        "# y_pred = clf.predict(X_test)\n",
        "# accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "# print(\"Dokadno: %0.3f\" % accur)\n",
        "# print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "# print(classification_report(y_test, y_pred))\n",
        "\n",
        "# print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "# print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# print(\"Sowa, kt贸re z najwiksz pewnoci wskazuj maszynie, 偶e wiadomo jest pozytywna:\")\n",
        "# top10 = np.argsort(clf.coef_[0])[-10:]\n",
        "# bottom10 = np.argsort(clf.coef_[0])[:10]\n",
        "# print(feature_names_text[top10])\n",
        "\n",
        "# print(\"Sowa najmniej istotne przy klasyfikacji:\")\n",
        "# print(feature_names_text[bottom10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqwF1_TlIIZj",
        "outputId": "ea21a9a0-5df9-440f-fb3a-cc7cb87a10cc"
      },
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.797\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       449\n",
            "           1       0.85      0.69      0.76       285\n",
            "           2       0.73      0.79      0.76       266\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.80      0.78      0.79      1000\n",
            "weighted avg       0.80      0.80      0.80      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[389  20  40]\n",
            " [ 51 197  37]\n",
            " [ 40  15 211]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Klasyfikator\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "accur = accuracy_score(y_test, y_pred) # dokadno\n",
        "print(\"Dokadno: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz bd贸w\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4JJu1XIMFp4",
        "outputId": "79338594-1799-4f56-e056-fd70776cb34a"
      },
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokadno: 0.797\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       449\n",
            "           1       0.85      0.69      0.76       285\n",
            "           2       0.73      0.79      0.76       266\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.80      0.78      0.79      1000\n",
            "weighted avg       0.80      0.80      0.80      1000\n",
            "\n",
            "Macierz bd贸w\n",
            "[[389  20  40]\n",
            " [ 51 197  37]\n",
            " [ 40  15 211]]\n"
          ]
        }
      ]
    }
  ]
}