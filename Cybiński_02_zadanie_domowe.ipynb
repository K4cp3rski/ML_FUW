{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K4cp3rski/ML_FUW/blob/master/Cybi%C5%84ski_02_zadanie_domowe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYtv4ZSbTx5X"
      },
      "source": [
        "# Praca domowa I, zadanie II\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaX60EphVC74"
      },
      "source": [
        "## Treść"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4_wGX3_coFa"
      },
      "source": [
        "### Wstęp fabularny"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJlu3kmQchcE"
      },
      "source": [
        "Wyobraź sobie, że jesteś pracownikiem w firmie sprzedającej kompleksową usługę tworzenia wizerunków medialnych. Oddział, w którym pracujesz obsługuje ważnego klienta działającego w branży gier i usług cyfrowych.\n",
        "\n",
        "Twoim zadaniem jest przygotować model uczenia maszynowego, który określać będzie nastawienie emocjonalne postów z Twittera. Zespół odpowiadający za zbieranie danych właśnie dostarczył zestaw danych dla Ciebie.\n",
        "\n",
        "Do tej pory klasyfikowaniem nastrojów z twittów zajmował się zespół ekspertów. Rozwiązanie takie jest bardzo wolne i drogie, a dokładność ekspertów wynosi tylko 95%. Dlatego zarząd firmy zlecił wdrożenie modelu uczenia maszynowego.\n",
        "\n",
        "Twój model stanowić będzie jedynie część większego produktu oferowanego przez Twoją firmę. Wyniki Twojego modelu będą bezpośrednio wykorzystywane przez następny zespół, którego zadaniem jest przygotować kolejny model uczenia maszynowego przewidujący reakcje opinii publicznej na posty klienta.\n",
        "\n",
        "Prace zespołu, który korzystać będzie z Twojego modelu są już bardzo zaawansowane, dlatego nie może on pozwolić sobie na żadne dodatkowe zmiany w swoim projekcie. Absolutnie konieczne jest, aby Twój model przyporządkowywał posty do jednej z trzech klas 'Positive', 'Negative', 'Neutral' lub analogicznych. Posty nie na temat powinny być klasyfikowane jako 'Neutral'.\n",
        "\n",
        "Notebook z Twoim projektem będzie oglądał Twój szef, więc koniecznie zadbaj, żeby znalazły się w nim najważniejsze przemyślenia, a rysunki były ładne.\n",
        "\n",
        "Powodzenia 🦾"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRk5odT-VpHl"
      },
      "source": [
        "### Polecenia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqep_SopVL6B"
      },
      "source": [
        "1. Wstępna obróbka danych:\n",
        "\n",
        " - załaduj zbiór treningowy i testowy,\n",
        " - usuń wiersze o brakujących elementach,\n",
        " - w kolumnie `sentiment` zamień wartości `'Irrelevant'` na `'Neutral'`.\n",
        "\n",
        "1. Wykonaj wizualizacje danych:\n",
        "\n",
        " - histogram tematów twittów (`entity`),\n",
        " - histogram nastawień (`sentiment`),\n",
        " - najczęściej padających słów w treści twittów (`content`).\n",
        "\n",
        "1. Przygotuj dane:\n",
        "\n",
        " - przygotuj zbiór cech poprzez wektoryzacje kolumny `content`, \n",
        " - przygotuj etykiety poprzez zakodowanie tekstowych wartości w kolumnie `sentiment` do postaci liczbowej.\n",
        "\n",
        "  Następnie wytrenuj naiwny model bayesowski. Sprawdź działanie modelu na kilku własnoręcznie napisanych wiadomościach. \n",
        "\n",
        "1. Wytrenuj modele:\n",
        " - naiwny bayesowski,\n",
        " - liniowy SVM,\n",
        " - regresji logistycznej,\n",
        " - drzewo decyzyjne.\n",
        "\n",
        "  Sprawdź model na danych treningowych (walidacja krzyżowa) i testowych, następnie wybierz najlepszy model. Uzasadnij swój wybór.\n",
        "  \n",
        "1. Zespół ekspertów ręcznie klasyfikuje dane z dokładnością 95%. Porównaj z nimi swój model i napisz jakie są przewagi Twojego modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZpaDghkU_B7"
      },
      "source": [
        "### Zbiór danych\n",
        "\n",
        "Zbiór danych został przygotowany na podstawie zbioru [Twitter Sentiment Analysis](https://www.kaggle.com/jp797498e/twitter-entity-sentiment-analysis) i składa się z dwóch plików:\n",
        "-  `twitter_training.csv` - zbiór treningowy,\n",
        "- `twitter_validation.csv` - zbiór testowy.\n",
        "\n",
        "Archiwum z plikami można pobrać z [dysku google](https://drive.google.com/file/d/1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB/view?usp=sharing) lub odkomentowując poniższe linie:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "lrlti4Zk4Lhf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VlvbsKdVU0hz"
      },
      "outputs": [],
      "source": [
        "# ! pip install gdown\n",
        "# ! gdown https://drive.google.com/uc?id=1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB\n",
        "# ! unzip twitter.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "95mTOShAjGji",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d79a6541-2b6a-44fb-f91a-8c13e2d78591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (366 kB)\n",
            "\u001b[K     |████████████████████████████████| 366 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from wordcloud) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\n",
            "Installing collected packages: wordcloud\n",
            "  Attempting uninstall: wordcloud\n",
            "    Found existing installation: wordcloud 1.5.0\n",
            "    Uninstalling wordcloud-1.5.0:\n",
            "      Successfully uninstalled wordcloud-1.5.0\n",
            "Successfully installed wordcloud-1.8.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Installing collected packages: regex, nltk\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.7 regex-2021.11.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "regex"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "Successfully installed scikit-learn-1.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 15.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U wordcloud\n",
        "!pip install -U nltk\n",
        "!pip install -U scikit-learn\n",
        "!pip install -U pandas\n",
        "!pip install -U seaborn\n",
        "!pip install -U numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGrsOK5pT1Xw"
      },
      "source": [
        "# Rozwiązanie"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impoty \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import cross_validate\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from scipy.sparse import hstack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TERrt4LEZdp",
        "outputId": "612c0dce-0de9-4aac-a290-0539d70cd7fa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ładowanie danych"
      ],
      "metadata": {
        "id": "de84o_Yt-3LL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lQ0m63_gU41z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1356d2-f2d2-4ee7-b694-ffbeea4719a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zainstalowana wersja scikit-learn: 1.0.1.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "print('Zainstalowana wersja scikit-learn: {}.'.format(sklearn.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10) # aby wykresy w Colabie były większe\n",
        "\n",
        "import numpy as np\n",
        "from scipy import diag, interp\n",
        "from itertools import cycle\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h6Ykp7XDRlF5"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('twitter_training.csv')\n",
        "test_data = pd.read_csv('twitter_validation.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7rWWbBA9wTr"
      },
      "source": [
        "Jak widzimy globalnie (Obrazek 1) klasy są w miare zbalansowane, liczebność nie różni się więcej jak 2-krotnie. W związku z tym wydaje się, że nie ma potrzeby sztucznego wyrównywania ich liczebności i można przejść do dalszego etapu preprocessingu danych"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing danych"
      ],
      "metadata": {
        "id": "uNbac6Dd_Do3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zmieniamy labelki na integerowe"
      ],
      "metadata": {
        "id": "FYYpHtacAvIs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aMIHVxWiR4s8"
      },
      "outputs": [],
      "source": [
        "# Przydzielamy klasom integerowe labelki\n",
        "\n",
        "# Neutral = Class 0\n",
        "train_data.loc[train_data['sentiment'] == 'Positive', 'sentiment'] = 0\n",
        "# Positive = Class 1\n",
        "train_data.loc[train_data['sentiment'] == 'Neutral', 'sentiment'] = 1\n",
        "# Negative = Class 2\n",
        "train_data.loc[train_data['sentiment'] == 'Negative', 'sentiment'] = 2\n",
        "# Irrelevant = Class 1\n",
        "train_data.loc[train_data['sentiment'] == 'Irrelevant', 'sentiment'] = 1\n",
        "\n",
        "# A teraz to samo dla zbiotu testowego\n",
        "\n",
        "# Neutral = Class 0\n",
        "test_data.loc[test_data['sentiment'] == 'Positive', 'sentiment'] = 0\n",
        "# Positive = Class 1\n",
        "test_data.loc[test_data['sentiment'] == 'Neutral', 'sentiment'] = 1\n",
        "# Negative = Class 2\n",
        "test_data.loc[test_data['sentiment'] == 'Negative', 'sentiment'] = 2\n",
        "# Irrelevant = Class 1\n",
        "test_data.loc[test_data['sentiment'] == 'Irrelevant', 'sentiment'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vYH-7SsoXZy1"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.drop(columns='id')\n",
        "test_data = test_data.drop(columns='id')\n",
        "\n",
        "train_data = train_data.rename(columns={'sentiment':'Class'})\n",
        "test_data = test_data.rename(columns={'sentiment':'Class'})\n",
        "\n",
        "# Pozbywamy się wierszy z niepełnymi informacjani (NaN)\n",
        "train_data = train_data.dropna(axis='rows', how='all', thresh=int(train_data.shape[1]))\n",
        "test_data = test_data.dropna(axis='rows', how='all', thresh=int(test_data.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wizualizacja danych, oraz najczęściej występujących słów w tweetach danych kategorii"
      ],
      "metadata": {
        "id": "dkfna3Sm_SYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Wizualizacja danych\n",
        "\n",
        "# f = sns.countplot(x='sentiment', data=train_data)\n",
        "# h = sns.catplot(y='entity', hue='sentiment', data=train_data, kind='count', height=20, aspect=1)\n",
        "# g = sns.catplot(x='sentiment', col='entity' , col_wrap=4, kind='count', data=train_data, height=4.5, aspect=1.2)\n",
        "# i = sns.catplot(y='entity', data=train_data, height=20, kind='count')\n",
        "\n",
        "# (g.set_axis_labels(\"\", \"Tweet count\")\n",
        "# .set_xticklabels([\"Positive\", \"Neutral\", \"Negative\", 'Irrelevant'])\n",
        "# .set_titles(\"{col_name}\")\n",
        "# .despine(left=True))  "
      ],
      "metadata": {
        "id": "fDucnE6j_P8J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RfeT0MkjjLdz"
      },
      "outputs": [],
      "source": [
        "# # Zobaczmy sobie najczęściej występujące słowa w klasach słów pozytywnych, neutralnych, negatywnych i irrelevant\n",
        "\n",
        "# import wordcloud\n",
        "# from wordcloud import WordCloud\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Neutralne tweety\n",
        "# neutral_list = list(map(str, train_data [train_data['Class'] == 0]['content']))\n",
        "\n",
        "# neutral_words = \" \".join(neutral_list)\n",
        "# neutral_plot = WordCloud(width = 512, height = 512).generate(neutral_words)\n",
        "\n",
        "# # Pozytywne tweety\n",
        "# positive_list = list(map(str, train_data [train_data['Class'] == 1]['content']))\n",
        "\n",
        "# positive_words = \" \".join(positive_list)\n",
        "# positive_plot = WordCloud(width = 512, height = 512).generate(positive_words)\n",
        "\n",
        "# # Negatywne tweety\n",
        "# negative_list = list(map(str, train_data [train_data['Class'] == 2]['content']))\n",
        "\n",
        "# negative_words = \" \".join(negative_list)\n",
        "# negative_plot = WordCloud(width = 512, height = 512).generate(negative_words)\n",
        "\n",
        "# plt.figure(figsize=(10,8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Iz2VYWJ-6KzW"
      },
      "outputs": [],
      "source": [
        "# images = [neutral_plot, positive_plot, negative_plot]\n",
        "# image_names = ['neutral', 'positive', 'negative']\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(20,5))\n",
        "# plt.suptitle('Most frequent words in tweets class-wise',y = 0.95, x = 0.4, weight='heavy', size='xx-large')\n",
        "# columns = 4\n",
        "# for i, image in enumerate(images):\n",
        "#     plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "#     plt.axis('off')\n",
        "#     plt.title(image_names[i])\n",
        "#     plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rozdzielamy na dane wejściowe i wyjściowe"
      ],
      "metadata": {
        "id": "Yigl7DLp_cHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Po tym zmieniamy komórki ze stringami na floaty za pomocą Vectorizera"
      ],
      "metadata": {
        "id": "37zZNe0A_n_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aO2ymm3i8gCy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train = train_data.drop(columns='Class')\n",
        "y_train = np.asarray(train_data['Class']).reshape(-1, 1).ravel()\n",
        "y_train = y_train.astype('int')\n",
        "\n",
        "X_test = test_data.drop(columns='Class')\n",
        "y_test = np.asarray(test_data['Class']).reshape(-1, 1).ravel()\n",
        "y_test = y_test.astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDYYgsO4HxKX"
      },
      "source": [
        "### Aby polepszyć wyniki klasyfikatorów poddajemy dane stemmingowi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmRzg4o6IVyu"
      },
      "source": [
        "Jednak jako, że jedna z naszych kolumn zawiera tylko nazwy gier/tematów o których jest pisane, to stemmingowi chcemy poddać jedynie kolumnę z treścią tweetów, bo to tam szukanie słów o wspólnych korzeniach znaczeniowych będzie mieć znaczenie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dP3zZ0aJKl77"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "words = stopwords.words(\"english\")\n",
        "train_data['cleaned'] = train_data['content'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
        "test_data['cleaned'] = test_data['content'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnOPvA2R8q2_"
      },
      "source": [
        "Teraz jak już mamy zwizualizowane dane i po stemmingu, to przekodujmy je na język zrozumiały przez maszynę, tj. wektory cech.\n",
        "\n",
        " W tym celu użyjemy funkcji *TfidfVectorizer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S-2GLc05AoU4"
      },
      "outputs": [],
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['cleaned']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['cleaned']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BSEn5jw--_rf"
      },
      "outputs": [],
      "source": [
        "# Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "vectorizer_text =  TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2)) # stwórz instancje obiektu CountVectorizer dla kodowania tekstu tweetów\n",
        "vectorizer_instances = CountVectorizer() # stwórz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna treści)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystając ze słownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna treści)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gW6D6c7Z_ul3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "285f404f-37d2-4474-d7cd-84f9b3265850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dane treningowe: n_samples: 73996, n_features: 103542\n"
          ]
        }
      ],
      "source": [
        "print(\"Dane treningowe: n_samples: %d, n_features: %d\" % X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii78Uj9REIrm"
      },
      "source": [
        "Odwrotne mapowanie cech na słowa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gPfcl7zBAMT0"
      },
      "outputs": [],
      "source": [
        "# # Odwrotne mapowanie tweetów na słowa\n",
        "\n",
        "# Dla kolumny z tekstem tweetów\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Byw6-3Kdg3k"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UelXhyVHdgUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b72f0c-d801-4f6f-a7cf-a0ee1f769d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokładność: 0.939\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.92      0.92       277\n",
            "           1       0.95      0.96      0.95       457\n",
            "           2       0.93      0.93      0.93       266\n",
            "\n",
            "    accuracy                           0.94      1000\n",
            "   macro avg       0.94      0.93      0.94      1000\n",
            "weighted avg       0.94      0.94      0.94      1000\n",
            "\n",
            "Macierz błędów\n",
            "[[254  14   9]\n",
            " [ 10 438   9]\n",
            " [  9  10 247]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model_NB = MultinomialNB()\n",
        "model_NB.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_NB.predict(X_test)\n",
        "accur = accuracy_score(y_test, y_pred) # dokładność\n",
        "print(\"Dokładność: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz błędów\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWYO9AXDgj-K"
      },
      "source": [
        "*** Inspiracja kodem z https://towardsdatascience.com/multi-class-text-classification-with-sklearn-and-nltk-in-python-a-software-engineering-use-case-779d4a28ba5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "z5z5foCldsFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c47a26-1e93-4e4f-dc54-18113e36cba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokładność: 1.000\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       277\n",
            "           1       1.00      1.00      1.00       457\n",
            "           2       1.00      1.00      1.00       266\n",
            "\n",
            "    accuracy                           1.00      1000\n",
            "   macro avg       1.00      1.00      1.00      1000\n",
            "weighted avg       1.00      1.00      1.00      1000\n",
            "\n",
            "Macierz błędów\n",
            "[[277   0   0]\n",
            " [  0 457   0]\n",
            " [  0   0 266]]\n",
            "MCC =  1.000, Balanced ACC = 1.000  \n"
          ]
        }
      ],
      "source": [
        "# instead of doing these steps one at a time, we can use a pipeline to complete them all at once\n",
        "pipeline = Pipeline([('chi',  SelectKBest(chi2, k=1200)),\n",
        "                     ('clf', MultinomialNB())])\n",
        "\n",
        "# fitting our model and save it in a pickle for later use\n",
        "model_NB = pipeline.fit(X_train, y_train)\n",
        "with open('Multinomial.pickle', 'wb') as f:\n",
        "    pickle.dump(model_NB, f)\n",
        "y_pred = np.array(y_test)\n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokładność\n",
        "print(\"Dokładność: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz błędów\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNgQ5EsoftTk"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JCatqNVlhBZw"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
        "model_SVM = LinearSVC(C=1E-2, dual=False, max_iter=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td3aryyPhDcG"
      },
      "source": [
        "Do SVM już musimy bezwzględnie przeskalować nasze dane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BJHTTOCTh-fd"
      },
      "outputs": [],
      "source": [
        "entities_train = train_data['entity']\n",
        "contents_train = train_data['cleaned']\n",
        "\n",
        "entities_test = test_data['entity']\n",
        "contents_test = test_data['cleaned']\n",
        "\n",
        "# Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "vectorizer_text =  TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2)) # stwórz instancje obiektu CountVectorizer dla kodowania tekstu tweetów\n",
        "vectorizer_instances = CountVectorizer() # stwórz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna instnacji)\n",
        "contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna treści)\n",
        "\n",
        "X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# Wektoryzowanie i przetransformowanie danych testowych korzystając ze słownika stworzonego na bazie danych treningowych\n",
        "\n",
        "entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna instnacji)\n",
        "contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna treści)\n",
        "\n",
        "X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# <--- !! ---> #\n",
        "\n",
        "\n",
        "\n",
        "# # Odwrotne mapowanie tweetów na słowa\n",
        "\n",
        "# Dla kolumny z tekstem tweetów\n",
        "\n",
        "feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# Dla kolumny z tematami\n",
        "\n",
        "feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "feature_names_instances = np.asarray(feature_names_instances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ejtax03Gf2Wx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc3d31c-85e9-478e-ff62-434bab594f96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=0.01, dual=False, max_iter=10000)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Fitowanie modelu (trwa ok 3 min)\n",
        "\n",
        "model_SVM.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVBxlbTPNhow",
        "outputId": "aac5f604-d934-4823-c4f3-d51d1b2a2311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokładność: 0.975\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97       277\n",
            "           1       0.98      0.98      0.98       457\n",
            "           2       0.97      0.97      0.97       266\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.97      0.97      0.97      1000\n",
            "weighted avg       0.98      0.97      0.97      1000\n",
            "\n",
            "Macierz błędów\n",
            "[[267   4   6]\n",
            " [  4 450   3]\n",
            " [  2   6 258]]\n",
            "MCC =  0.961, Balanced ACC = 0.973  \n"
          ]
        }
      ],
      "source": [
        "# Walidacja \n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "y_pred = model_SVM.predict(X_test)\n",
        "accur = accuracy_score(y_test, y_pred) # dokładność\n",
        "print(\"Dokładność: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz błędów\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muNxoVJmtvYe"
      },
      "source": [
        "Dostajemy bardzo dobry wynik, skuteczność na poziomie 97%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz3j0lMrtYsA"
      },
      "source": [
        "## Regresja Logistyczna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MxDvZTa-t7Zd"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Tu znowu model liczy się ok 3 min\n",
        "\n",
        "# Teraz pewnie z 20 min\n",
        "model_log = LogisticRegression(solver = 'lbfgs', max_iter=10000, C=0.1)\n",
        "\n",
        "# W wyniku tego szukania wybrano C=0.1\n",
        "\n",
        "# grid_values = {'penalty': ['l2'], 'C': [0.1,1,10]}\n",
        "# model_log = GridSearchCV(model_log, param_grid=grid_values, n_jobs=-1, verbose=4)\n",
        "\n",
        "model_log.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YxNwr1LPv54m"
      },
      "outputs": [],
      "source": [
        "# entities_train = train_data['entity']\n",
        "# contents_train = train_data['cleaned']\n",
        "\n",
        "# entities_test = test_data['entity']\n",
        "# contents_test = test_data['cleaned']\n",
        "\n",
        "# # Dopasowanie i wektoryzowanie dla danych treningowych wraz ze stemmingiem\n",
        "\n",
        "# vectorizer_text =  TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2)) # stwórz instancje obiektu CountVectorizer dla kodowania tekstu tweetów\n",
        "# vectorizer_instances = CountVectorizer() # stwórz instancje obiektu CountVectorizer dla kodowania instancji\n",
        "# entities_train = vectorizer_instances.fit_transform(entities_train) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna instnacji)\n",
        "# contents_train = vectorizer_text.fit_transform(contents_train) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna treści)\n",
        "\n",
        "# X_train = hstack((entities_train, contents_train))\n",
        "\n",
        "# # Wektoryzowanie i przetransformowanie danych testowych korzystając ze słownika stworzonego na bazie danych treningowych\n",
        "\n",
        "# entities_test = vectorizer_instances.transform(entities_test) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna instnacji)\n",
        "# contents_test = vectorizer_text.transform(contents_test) # naucz vectorizer słownika i przetransformuj dane uczące (kolumna treści)\n",
        "\n",
        "# X_test = hstack((entities_test, contents_test))\n",
        "\n",
        "\n",
        "# scaler = StandardScaler(with_mean=False)\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# # <--- !! ---> #\n",
        "\n",
        "\n",
        "\n",
        "# # # Odwrotne mapowanie tweetów na słowa\n",
        "\n",
        "# # Dla kolumny z tekstem tweetów\n",
        "\n",
        "# feature_names_text = vectorizer_text.get_feature_names_out()\n",
        "# feature_names_text = np.asarray(feature_names_text)\n",
        "\n",
        "# # Dla kolumny z tematami\n",
        "\n",
        "# feature_names_instances = vectorizer_instances.get_feature_names_out()\n",
        "# feature_names_instances = np.asarray(feature_names_instances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "aG-o5Wuju3mY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84e9647-e1c3-418e-dfef-2ac5acd8bbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokładność: 0.975\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97       277\n",
            "           1       0.98      0.98      0.98       457\n",
            "           2       0.96      0.97      0.97       266\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.97      0.97      0.97      1000\n",
            "weighted avg       0.98      0.97      0.98      1000\n",
            "\n",
            "Macierz błędów\n",
            "[[268   3   6]\n",
            " [  5 448   4]\n",
            " [  3   4 259]]\n",
            "MCC =  0.961, Balanced ACC = 0.974  \n"
          ]
        }
      ],
      "source": [
        "y_pred = model_log.predict(X_test)  \n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokładność\n",
        "print(\"Dokładność: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz błędów\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation\n",
        "\n",
        "scores = cross_validate(model_log, X_train, y_train, n_jobs=-1, scoring='accuracy', verbose=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnqPk0Cu94BB",
        "outputId": "8a74a70f-a1fc-48ce-a118-907c62377b1b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.1min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr3VyiT2yfwD"
      },
      "source": [
        "## DecissionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "n2UslwUeyi2W"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "\n",
        "# Deklarujemy klasyfikator\n",
        "model_tree = tree.DecisionTreeClassifier()\n",
        "# Fitujemy do danych treningowych\n",
        "model_tree = model_tree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wvSOoiM5zHOy"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import Image \n",
        "\n",
        "# from six import StringIO\n",
        "# import pydot \n",
        "\n",
        "# dot_data = StringIO()  \n",
        "# tree.export_graphviz(clf, out_file=dot_data,  \n",
        "                         # class_names=['Neutral', 'Positive', 'Negative'],  \n",
        "                         # filled=True, rounded=True,  \n",
        "                         # special_characters=True) \n",
        "# graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
        "# Image(graph[0].create_png())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_tree.predict(X_test)  \n",
        "\n",
        "accur = accuracy_score(y_test, y_pred) # dokładność\n",
        "print(\"Dokładność: %0.3f\" % accur)\n",
        "print(\"Classification report:\") # wypisz raport klasyfikacji \n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Macierz błędów\") # wypisz macierz (confusion matrix)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "MCC = metrics.matthews_corrcoef(y_test, y_pred)\n",
        "BACC = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "print('MCC =  {m:.3f}, Balanced ACC = {b:.3f}  '.format(m=MCC, b=BACC))"
      ],
      "metadata": {
        "id": "j-QAj8bP4Bkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb6d158-4f7f-4496-9151-456debc7a784"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokładność: 0.924\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91       277\n",
            "           1       0.94      0.92      0.93       457\n",
            "           2       0.89      0.95      0.92       266\n",
            "\n",
            "    accuracy                           0.92      1000\n",
            "   macro avg       0.92      0.93      0.92      1000\n",
            "weighted avg       0.92      0.92      0.92      1000\n",
            "\n",
            "Macierz błędów\n",
            "[[249  17  11]\n",
            " [ 17 421  19]\n",
            " [  4   8 254]]\n",
            "MCC =  0.883, Balanced ACC = 0.925  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Elapsed time\\n--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "RwRVasC-4JW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fe08da-403e-4bb3-d94a-6025c46ed967"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time\n",
            "--- 488.72840332984924 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bez CV\n",
        "\n",
        "Elapsed time\n",
        "--- 472.361172914505 seconds ---"
      ],
      "metadata": {
        "id": "VzuVKqZX9rcf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Cybiński_02_zadanie_domowe.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}